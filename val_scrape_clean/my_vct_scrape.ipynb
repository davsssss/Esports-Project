{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO SAVE ALL MATCH DATA IN ONE FILE\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_and_append_tables(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Use the class selector to find all tables with the specified class\n",
    "        tables = soup.find_all('table', class_='wf-table-inset mod-overview')\n",
    "\n",
    "        # Initialize an empty list to store DataFrames\n",
    "        dfs = []\n",
    "\n",
    "        # Convert each table to a DataFrame and append to the list\n",
    "        for table in tables:\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            dfs.append(df)\n",
    "\n",
    "        # If there are no tables, return None\n",
    "        if not dfs:\n",
    "            return None\n",
    "\n",
    "        # Read the existing CSV file if it exists, or create an empty DataFrame\n",
    "        try:\n",
    "            existing_df = pd.read_csv(\"2023_vct_all_matches.csv\")\n",
    "        except FileNotFoundError:\n",
    "            existing_df = pd.DataFrame()\n",
    "\n",
    "        # Concatenate the existing DataFrame with the new tables\n",
    "        updated_df = pd.concat([existing_df] + dfs, ignore_index=True)\n",
    "\n",
    "        # Save the updated DataFrame to the CSV file\n",
    "        updated_df.to_csv(\"2023_vct_all_matches.csv\", index=False)\n",
    "\n",
    "        return updated_df\n",
    "\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(f\"HTTP Error ({errh.response.status_code}): {errh}\")\n",
    "        print(f\"Skipping {url} due to an error.\\n\")\n",
    "        return None\n",
    "\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Something went wrong: {err}\")\n",
    "        print(f\"Skipping {url} due to an error.\\n\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO EXTRACT ALL HREF LINKS\n",
    "\n",
    "def extract_href_links(base_url):\n",
    "    href_links = []\n",
    "\n",
    "    # Start at the initial URL\n",
    "    current_url = base_url\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Make a request to the current URL\n",
    "            response = requests.get(current_url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract href links and add them to the list\n",
    "            href_links.extend([a['href'] for a in soup.find_all('a', class_='wf-module-item')])\n",
    "\n",
    "            # Find the next page link (if any)\n",
    "            next_page_link = soup.find('a', class_='next-page')\n",
    "            if next_page_link:\n",
    "                # Update the current URL for the next iteration\n",
    "                current_url = f\"{base_url}/{next_page_link['href']}\"\n",
    "            else:\n",
    "                break  # Exit the loop if there is no next page link\n",
    "\n",
    "        except requests.exceptions.HTTPError as errh:\n",
    "            print(f\"HTTP Error ({errh.response.status_code}): {errh}\")\n",
    "            break  # Exit the loop if an HTTP error occurs\n",
    "\n",
    "        except requests.exceptions.RequestException as err:\n",
    "            print(f\"Something went wrong: {err}\")\n",
    "            break  # Exit the loop if a general error occurs\n",
    "\n",
    "    return href_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to access: https://www.vlr.gg/167391/loud-vs-drx-champions-tour-2023-lock-in-s-o-paulo-sf\n",
      "Data from https://www.vlr.gg/167391/loud-vs-drx-champions-tour-2023-lock-in-s-o-paulo-sf appended to '2023_vct_all_matches.csv'.\n",
      "\n",
      "Attempting to access: https://www.vlr.gg/167392/natus-vincere-vs-fnatic-champions-tour-2023-lock-in-s-o-paulo-sf\n",
      "Data from https://www.vlr.gg/167392/natus-vincere-vs-fnatic-champions-tour-2023-lock-in-s-o-paulo-sf appended to '2023_vct_all_matches.csv'.\n",
      "\n",
      "Attempting to access: https://www.vlr.gg/167393/loud-vs-fnatic-champions-tour-2023-lock-in-s-o-paulo-gf\n",
      "Data from https://www.vlr.gg/167393/loud-vs-fnatic-champions-tour-2023-lock-in-s-o-paulo-gf appended to '2023_vct_all_matches.csv'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "base_url = 'https://www.vlr.gg/event/matches/1188/champions-tour-2023-lock-in-s-o-paulo/?series_id=2756'\n",
    "#change url for data needed\n",
    "#base_url = 'https://www.vlr.gg/event/matches/1657/valorant-champions-2023/?series_id=all'\n",
    "href_links = extract_href_links(base_url)\n",
    "\n",
    "# Process the extracted href links\n",
    "for i, href_link in enumerate(href_links):\n",
    "    full_url = f\"https://www.vlr.gg{href_link}\"\n",
    "    print(f\"Attempting to access: {full_url}\")\n",
    "\n",
    "    result_df = scrape_and_append_tables(full_url)\n",
    "\n",
    "    if result_df is not None:\n",
    "        print(f\"Data from {full_url} appended to '2023_vct_all_matches.csv'.\\n\")\n",
    "\n",
    "    # Add a delay between requests\n",
    "    time.sleep(1)  # Adjust the delay as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
